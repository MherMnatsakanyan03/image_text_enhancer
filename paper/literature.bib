@inproceedings{adaptive_thresholding_methods_bataineh_2011,
author = {Bataineh, Bilal and Abdullah, Siti and Omar, Khairuddin and Nasrudin, Mohammad Faidzul},
year = {2011},
month = {06},
pages = {230-239},
title = {Adaptive Thresholding Methods for Documents Image Binarization},
volume = {6718},
isbn = {978-3-642-21586-5},
doi = {10.1007/978-3-642-21587-2_25},
url={https://www.researchgate.net/publication/220827321_Adaptive_Thresholding_Methods_for_Documents_Image_Binarization}
}

@article{adaptive_thresholding_integral_image_bradley_roth_2007,
author = {Bradley, Derek and Roth, Gerhard},
year = {2007},
month = {01},
pages = {13-21},
title = {Adaptive Thresholding using the Integral Image},
volume = {12},
journal = {J. Graphics Tools},
doi = {10.1080/2151237X.2007.10129236},
url={https://doi.org/10.1080/2151237X.2007.10129236}
}

@inproceedin{Binarization_historical_document_images_local_maximum_minimum_su_2010,
  author = {Su, Bolan and Lu, Shijian and Tan, Chew Lim},
  title = {Binarization of historical document images using the local maximum and   minimum},
  year = {2010},
  isbn = {9781605587738},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1815330.1815351},
  abstract = {This paper presents a new document image binarization technique that   segments the text from badly degraded historical document images. The proposed   technique makes use of the image contrast that is defined by the local image   maximum and minimum. Compared with the image gradient, the image contrast   evaluated by the local maximum and minimum has a nice property that it is more   tolerant to the uneven illumination and other types of document degradation such   as smear. Given a historical document image, the proposed technique first   constructs a contrast image and then detects the high contrast image pixels which   usually lie around the text stroke boundary. The document text is then segmented   by using local thresholds that are estimated from the detected high contrast   pixels within a local neighborhood window. The proposed technique has been tested   over the dataset that is used in the recent Document Image Binarization Contest   (DIBCO) 2009. Experiments show its superior performance.},
  booktitle = {Proceedings of the 9th IAPR International Workshop on Document   Analysis Systems},
  pages = {159–166},
  numpages = {8},
  keywords = {document image analysis, document image binarization, image contrast,   image pixel classification},
  location = {Boston, Massachusetts, USA},
  series = {DAS '10},
  url={https://doi.org/10.1145/1815330.1815351}
}

@techreport{ITU-R_BT601,
  author       = {{International Telecommunication Union}},
  title        = {Recommendation ITU-R BT.601-7: Studio encoding parameters of digital television for standard 4:3 and wide-screen 16:9 aspect ratios},
  institution  = {ITU Radiocommunication Sector (ITU-R)},
  year         = {2011},
  number       = {BT.601-7},
  type         = {Recommendation},
  address      = {Geneva, Switzerland},
  url={https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.601-7-201103-I!!PDF-E.pdf}
}

@article{image_binarization_sauvola_2000,
  title = {Adaptive document image binarization},
  journal = {Pattern Recognition},
  volume = {33},
  number = {2},
  pages = {225-236},
  year = {2000},
  issn = {0031-3203},
  doi = {https://doi.org/10.1016/S0031-3203(99)00055-2},
  author = {J. Sauvola and M. Pietikäinen},
  keywords = {Adaptive binarization, Soft decision, Document segmentation, Document   analysis, Document understanding},
  abstract = {A new method is presented for adaptive document image binarization,   where the page is considered as a collection of subcomponents such as text,   background and picture. The problems caused by noise, illumination and many source   type-related degradations are addressed. Two new algorithms are applied to   determine a local threshold for each pixel. The performance evaluation of the   algorithm utilizes test images with ground-truth, evaluation metrics for   binarization of textual and synthetic images, and a weight-based ranking procedure   for the final result presentation. The proposed algorithms were tested with images   including different types of document components and degradations. The results   were compared with a number of known techniques in the literature. The   benchmarking results show that the method adapts and performs well in each case   qualitatively and quantitatively.},
  url={https://www.sciencedirect.com/science/article/pii/S0031320399000552}
}

@INPROCEEDINGS{voting_method_image_binarization_vlasceanu_2022,
  author={Vlăsceanu, Giorgiana Violeta and Ghenadie, Caraman and Niţu, Răzvan and Boiangiu, Costin-Anton},
  booktitle={2022 21st RoEduNet Conference: Networking in Education and Research (RoEduNet)}, 
  title={A voting method for image binarization of text-based documents}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  keywords={Measurement;Image analysis;Text analysis;Runtime;Text recognition;Education;Filtering algorithms;binarization;voting system;thresholding;document image analysis},
  doi={10.1109/RoEduNet57163.2022.9921086},
  url={https://ieeexplore.ieee.org/document/9921086}
}


@Article{review_document_binarization_yang_2024,
  AUTHOR = {Yang, Zhengxian and Zuo, Shikai and Zhou, Yanxi and He, Jinlong andShi, Jianwen},
  TITLE = {A Review of Document Binarization: Main Techniques, New Challenges,and Trends},
  JOURNAL = {Electronics},
  VOLUME = {13},
  YEAR = {2024},
  NUMBER = {7},
  ARTICLE-NUMBER = {1394},
  ISSN = {2079-9292},
  ABSTRACT = {Document image binarization is a challenging task, especially whenit comes to text segmentation in degraded document images. The binarization, asa pre-processing step of Optical Character Recognition (OCR), is one of themost fundamental and commonly used segmentation methods. It separates theforeground text from the background of the document image to facilitatesubsequent image processing. In view of the different degradation degrees ofdocument images, researchers have proposed a variety of solutions. In thispaper, we have summarized some challenges and difficulties in the field ofdocument image binarization. Approximately 60 methods documenting imagebinarization techniques are mentioned, including traditional algorithms anddeep learning-based algorithms. Here, we evaluated the performance of 25 imagebinarization techniques on the H-DIBCO2016 dataset to provide some help forfuture research.},
  DOI = {10.3390/electronics13071394},
  url={https://www.mdpi.com/2079-9292/13/7/1394}
}


@Article{review_degradation_image_enhancement_zhou_2023,
AUTHOR = {Zhou, Yanxi and Zuo, Shikai and Yang, Zhengxian and He, Jinlong and Shi, Jianwen and Zhang, Rui},
TITLE = {A Review of Document Image Enhancement Based on Document Degradation Problem},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {13},
ARTICLE-NUMBER = {7855},
ISSN = {2076-3417},
ABSTRACT = {Document image enhancement methods are often used to improve the accuracy and efficiency of automated document analysis and recognition tasks such as character recognition. These document images could be degraded or damaged for various reasons including aging, fading handwriting, poor lighting conditions, watermarks, etc. In recent years, with the improvement of computer performance and the continuous development of deep learning, many methods have been proposed to enhance the quality of these document images. In this paper, we review six tasks of document degradation, namely, background texture, page smudging, fading, poor lighting conditions, watermarking, and blurring. We summarize the main models for each degradation problem as well as recent work, such as the binarization model that can be used to deal with the degradation of background textures, lettering smudges. When facing the problem of fading, a model for stroke connectivity can be used, while the other three degradation problems are mostly deep learning models. We discuss the current limitations and challenges of each degradation task and introduce the common public datasets and metrics. We identify several promising research directions and opportunities for future research.},
DOI = {10.3390/app13137855},
url={https://www.mdpi.com/2076-3417/13/13/7855}
}

@INPROCEEDINGS{Investigation_binarization_techniques_unevenly_illuminated_document_images_alqudah_2015,
  author={Alqudah, Musab Kasim and Bin Nasrudin, Mohammad F. and Bataineh, Bilal and Alqudah, Mashal and Alkhatatneh, Arwa},
  booktitle={2015 International Conference on Computer, Communications, and Control Technology (I4CT)}, 
  title={Investigation of binarization techniques for unevenly illuminated document images acquired via handheld cameras}, 
  year={2015},
  volume={},
  number={},
  pages={524-529},
  keywords={Lighting;Visualization;Standards;Cameras;Benchmark testing;PSNR;Document Image Benchmark;Hand-held Camera;Local Binarization;Global Binarization;Thresholding;Uneven Illumination},
  doi={10.1109/I4CT.2015.7219634},
  url={https://ieeexplore.ieee.org/document/7219634}
}

@misc{new_local_adaptive_thresholding_romen_singh_2012,
      title={A New Local Adaptive Thresholding Technique in Binarization}, 
      author={T. Romen Singh and Sudipta Roy and O. Imocha Singh and Tejmani Sinam and Kh. Manglem Singh},
      year={2012},
      eprint={1201.5227},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1201.5227}, 
}


@Article{image_binarization_local_entropy_filtering_michalak_okarma_2019,
AUTHOR = {Michalak, Hubert and Okarma, Krzysztof},
TITLE = {Improvement of Image Binarization Methods Using Image Preprocessing with Local Entropy Filtering for Alphanumerical Character Recognition Purposes},
JOURNAL = {Entropy},
VOLUME = {21},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {562},
PubMedID = {33267276},
ISSN = {1099-4300},
ABSTRACT = {Automatic text recognition from the natural images acquired in uncontrolled lighting conditions is a challenging task due to the presence of shadows hindering the shape analysis and classification of individual characters. Since the optical character recognition methods require prior image binarization, the application of classical global thresholding methods in such case makes it impossible to preserve the visibility of all characters. Nevertheless, the use of adaptive binarization does not always lead to satisfactory results for heavily unevenly illuminated document images. In this paper, the image preprocessing methodology with the use of local image entropy filtering is proposed, allowing for the improvement of various commonly used image thresholding methods, which can be useful also for text recognition purposes. The proposed approach was verified using a dataset of 140 differently illuminated document images subjected to further text recognition. Experimental results, expressed as Levenshtein distances and F-Measure values for obtained text strings, are promising and confirm the usefulness of the proposed approach.},
DOI = {10.3390/e21060562},
URL = {https://www.mdpi.com/1099-4300/21/6/562}
}

@INPROCEEDINGS{image_binarization_end_to_end_text_understanding_milyaev_2013,
  author={Milyaev, Sergey and Barinova, Olga and Novikova, Tatiana and Kohli, Pushmeet and Lempitsky, Victor},
  booktitle={2013 12th International Conference on Document Analysis and Recognition}, 
  title={Image Binarization for End-to-End Text Understanding in Natural Images}, 
  year={2013},
  volume={},
  number={},
  pages={128-132},
  keywords={Optical character recognition software;Accuracy;Text recognition;Engines;Image segmentation;Pipelines;Image recognition;natural scene binarization;text localization},
  doi={10.1109/ICDAR.2013.33}}

@ARTICLE{Otsu1979,
  author={Otsu, Nobuyuki},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={A Threshold Selection Method from Gray-Level Histograms}, 
  year={1979},
  volume={9},
  number={1},
  pages={62-66},
  keywords={Histograms;Marine vehicles;Radar tracking;Least squares approximation;Surveillance;Target tracking;Gaussian distribution;Displays;Q measurement;Sea measurements},
  doi={10.1109/TSMC.1979.4310076},
  URL={https://ieeexplore.ieee.org/document/4310076}
  }

@INPROCEEDINGS{Yang2008,
  author={Yang, You},
  booktitle={2008 Congress on Image and Signal Processing}, 
  title={OCR Oriented Binarization Method of Document Image}, 
  year={2008},
  volume={4},
  number={},
  pages={622-625},
  keywords={Optical character recognition software;Computer science;Government;Image segmentation;Pixel;Smoothing methods;Digital signal processing;Laboratories;Space technology;Mathematics;binarization;document image;OCR},
  doi={10.1109/CISP.2008.262},
  URL={https://ieeexplore.ieee.org/document/4566727}
  }


@article{Lu2010,
	abstract = {Document images often suffer from different types of degradation that renders the document image binarization a challenging task. This paper presents a document image binarization technique that segments the text from badly degraded document images accurately. The proposed technique is based on the observations that the text documents usually have a document background of the uniform color and texture and the document text within it has a different intensity level compared with the surrounding document background. Given a document image, the proposed technique first estimates a document background surface through an iterative polynomial smoothing procedure. Different types of document degradation are then compensated by using the estimated document background surface. The text stroke edge is further detected from the compensated document image by using L1-norm image gradient. Finally, the document text is segmented by a local threshold that is estimated based on the detected text stroke edges. The proposed technique was submitted to the recent document image binarization contest (DIBCO) held under the framework of ICDAR 2009 and has achieved the top performance among 43 algorithms that are submitted from 35 international research groups.},
	author = {Lu, Shijian and Su, Bolan and Tan, Chew Lim},
	date = {2010/12/01},
	date-added = {2026-01-18 13:28:30 +0100},
	date-modified = {2026-01-18 13:28:30 +0100},
	doi = {10.1007/s10032-010-0130-8},
	id = {Lu2010},
	isbn = {1433-2825},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	number = {4},
	pages = {303--314},
	title = {Document image binarization using background estimation and stroke edges},
	url = {https://doi.org/10.1007/s10032-010-0130-8},
	volume = {13},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1007/s10032-010-0130-8}}

@INPROCEEDINGS{Tong2009,
  author={Tong, Li-Jing and Chen, Kan and Zhang, Yan and Fu, Xiao-Ling and Duan, Jian-Yong},
  booktitle={2009 2nd International Congress on Image and Signal Processing}, 
  title={Document Image Binarization Based on NFCM}, 
  year={2009},
  volume={},
  number={},
  pages={1-5},
  keywords={Clustering algorithms;Optical character recognition software;Histograms;Cameras;Image segmentation;Image recognition;Lighting;Brightness;Character recognition;Lenses},
  doi={10.1109/CISP.2009.5305330}}


@Article{comprehensive_review_document_image_binarization_bataineh_2025,
AUTHOR = {Bataineh, Bilal and Tounsi, Mohamed and Zamzami, Nuha and Janbi, Jehan and Abu-ain, Waleed Abdel Karim and AbuAin, Tarik and Elnazer, Shaima},
TITLE = {A Comprehensive Review on Document Image Binarization},
JOURNAL = {Journal of Imaging},
VOLUME = {11},
YEAR = {2025},
NUMBER = {5},
ARTICLE-NUMBER = {133},
PubMedID = {40422990},
ISSN = {2313-433X},
ABSTRACT = {In today’s digital age, the conversion of hardcopy documents into digital formats is widespread. This process involves electronically scanning and storing large volumes of documents. These documents come from various sources, including records and reports, camera-captured text and screen snapshots, official documents, newspapers, medical reports, music scores, and more. In the domain of document analysis techniques, an essential step is document image binarization. Its goal is to eliminate unnecessary data from images and preserve only the text. Despite the existence of multiple techniques for binarization, the presence of degradation in document images can hinder their efficacy. The objective of this work is to provide an extensive review and analysis of the document binarization field, emphasizing its importance and addressing the challenges encountered during the image binarization process. Additionally, it provides insights into techniques and methods employed for image binarization. The current paper also introduces benchmark datasets for evaluating binarization accuracy, model training, evaluation metrics, and the effectiveness of recent methods.},
DOI = {10.3390/jimaging11050133},
URL = {https://www.mdpi.com/2313-433X/11/5/133}
}

@article{automated_entry_system_printed_documents_akiyama_hagita_1990,
title = {Automated entry system for printed documents},
journal = {Pattern Recognition},
volume = {23},
number = {11},
pages = {1141-1154},
year = {1990},
issn = {0031-3203},
doi = {https://doi.org/10.1016/0031-3203(90)90112-X},
author = {Teruo Akiyama and Norihiro Hagita},
keywords = {Document entry system, Image processing, Document processing, Layout structure recognition, Character recognition, Feature extraction, Character segmentation},
abstract = {This paper proposes a system for automatically reading either Japanese or English documents that have complex layout structures that include graphics. First, document image segmentation and character segmentation are carried out using three basic features and the knowledge of document layout rules. Next, multi-font character recognition is performed based on feature vector matching. Recognition experiments with a prototype system for a variety of complex printed documents shows that the proposed system is capable of reading different types of printed documents at an accuracy rate of 94.8–97.2%.},
url = {https://www.sciencedirect.com/science/article/pii/003132039090112X}
}


@Article{novel_adaptive_deskewing_algorithm_document_images_bao_2022,
AUTHOR = {Bao, Wuzhida and Yang, Cihui and Wen, Shiping and Zeng, Mengjie and Guo, Jianyong and Zhong, Jingting and Xu, Xingmiao},
TITLE = {A Novel Adaptive Deskewing Algorithm for Document Images},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {20},
ARTICLE-NUMBER = {7944},
PubMedID = {36298294},
ISSN = {1424-8220},
ABSTRACT = {Document scanning often suffers from skewing, which may seriously influence the efficiency of Optical Character Recognition (OCR). Therefore, it is necessary to correct the skewed document before document image information analysis. In this article, we propose a novel adaptive deskewing algorithm for document images, which mainly includes Skeleton Line Detection (SKLD), Piecewise Projection Profile (PPP), Morphological Clustering (MC), and the image classification method. The image type is determined firstly based on the image’s layout feature. Thus, adaptive correcting is applied to deskew the image according to its type. Our method maintains high accuracy on the Document Image Skew Estimation Contest (DISEC’2013) and PubLayNet datasets, which achieved 97.6% and 80.1% accuracy, respectively. Meanwhile, extensive experiments show the superiority of the proposed algorithm.},
DOI = {10.3390/s22207944},
URL = {https://www.mdpi.com/1424-8220/22/20/7944}
}

@article{review_image_denoising_techniques_fan_2019,
	abstract = {With the explosion in the number of digital images taken every day, the demand for more accurate and visually pleasing images is increasing. However, the images captured by modern cameras are inevitably degraded by noise, which leads to deteriorated visual image quality. Therefore, work is required to reduce noise without losing image features (edges, corners, and other sharp structures). So far, researchers have already proposed various methods for decreasing noise. Each method has its own advantages and disadvantages. In this paper, we summarize some important research in the field of image denoising. First, we give the formulation of the image denoising problem, and then we present several image denoising techniques. In addition, we discuss the characteristics of these techniques. Finally, we provide several promising directions for future research.},
	author = {Fan, Linwei and Zhang, Fan and Fan, Hui and Zhang, Caiming},
	date = {2019/07/08},
	date-added = {2026-01-19 09:17:05 +0100},
	date-modified = {2026-01-19 09:17:05 +0100},
	doi = {10.1186/s42492-019-0016-7},
	id = {Fan2019},
	isbn = {2524-4442},
	journal = {Visual Computing for Industry, Biomedicine, and Art},
	number = {1},
	pages = {7},
	title = {Brief review of image denoising techniques},
	volume = {2},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1186/s42492-019-0016-7},
  url = {https://doi.org/10.1186/s42492-019-0016-7}}

@misc{cimg,
title = {CImg: A Simple C++ Toolkit for Image Processing},
author = {Tarel, Jean-Philippe},
year = {2024},
howpublished = {\url{https://cimg.eu}},
note = {Version 3.x}
}

@book{Image_engineering_zhang_2017,
author = {Zhang, Yu-Jin},
address = {Berlin},
booktitle = {Image engineering},
isbn = {9783110520323},
keywords = {Image processing ; image processing},
lccn = {2017289411},
publisher = {De Gruyter},
series = {De Gruyter graduate},
title = {Image engineering },
year = {2017},
}

@book{image_engineering_vl2_zhang_2017,
publisher = {De Gruyter,},
series = {De Gruyter Textbook},
title = {Image Engineering. Vol 2, Image Analysis },
abstract = {This graduate textbook presents fundamentals, applications and evaluation of image segregation, unit description, feature measurement and pattern recognition. Analysis on textile, shape and motion are discussed and mathematical tools are employed extensively. Rich in examples and excises, it prepares electrical engineering and computer science students with knowledge and skills for further studies on image understanding.},
author = {Zhang, Yujin and Tsinghua University Press.},
address = {Berlin ;},
booktitle = {Image Engineering.  Vol 2,  Image Analysis},
edition = {1st ed.},
isbn = {9783110524284},
keywords = {Image processing},
year = {2017},
}

@book{Digitale_Bildverarbeitung_werner_2020,
  author = {Werner, Martin},
  publisher = {Springer Vieweg Wiesbaden},
  title = {Digitale Bildverarbeitung},
  subtitle = {Grundkurs mit neuronalen Netzen und MATLAB®-Praktikum},
  year = {2020},
  isbn = {978-3-658-22184-3},
  doi = {https://doi.org/10.1007/978-3-658-22185-0},
  url = {https://link.springer.com/book/10.1007/978-3-658-22185-0},
}

@article{connected_components_hesselink_meijster_bron_2001,
title = {Concurrent determination of connected components},
journal = {Science of Computer Programming},
volume = {41},
number = {2},
pages = {173-194},
year = {2001},
issn = {0167-6423},
doi = {https://doi.org/10.1016/S0167-6423(01)00007-7},
author = {Wim H. Hesselink and Arnold Meijster and Coenraad Bron},
keywords = {Connected components, Parallel algorithm, Pthreads, Mutex, Condition variable},
abstract = {The design is described of a parallel version of Tarjan's algorithm for the determination of equivalence classes in graphs that represent images. Distribution of the vertices of the graph over a number of processes leads to a message passing algorithm. The algorithm is mapped to a shared-memory architecture by means of POSIX threads. It is applied to the determination of connected components in image processing. Experiments show a satisfactory speedup for sufficiently large images.},
url = {https://www.sciencedirect.com/science/article/pii/S0167642301000077}
}


@Article{degraded_historical_document_binarization_sulaiman_2019,
AUTHOR = {Sulaiman, Alaa and Omar, Khairuddin and Nasrudin, Mohammad F.},
TITLE = {Degraded Historical Document Binarization: A Review on Issues, Challenges, Techniques, and Future Directions},
JOURNAL = {Journal of Imaging},
VOLUME = {5},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {48},
PubMedID = {34460486},
ISSN = {2313-433X},
ABSTRACT = {In this era of digitization, most hardcopy documents are being transformed into digital formats. In the process of transformation, large quantities of documents are stored and preserved through electronic scanning. These documents are available from various sources such as ancient documentation, old legal records, medical reports, music scores, palm leaf, and reports on security-related issues. In particular, ancient and historical documents are hard to read due to their degradation in terms of low contrast and existence of corrupted artefacts. In recent times, degraded document binarization has been studied widely and several approaches were developed to deal with issues and challenges in document binarization. In this paper, a comprehensive review is conducted on the issues and challenges faced during the image binarization process, followed by insights on various methods used for image binarization. This paper also discusses the advanced methods used for the enhancement of degraded documents that improves the quality of documents during the binarization process. Further discussions are made on the effectiveness and robustness of existing methods, and there is still a scope to develop a hybrid approach that can deal with degraded document binarization more effectively.},
DOI = {10.3390/jimaging5040048},
URL = {https://www.mdpi.com/2313-433X/5/4/48}
}



@inproceedings{exploring_web_assembly_in_hpc_chadha_2023,
author = {Chadha, Mohak and Krueger, Nils and John, Jophin and Jindal, Anshul and Gerndt, Michael and Benedict, Shajulin},
title = {Exploring the Use of WebAssembly in HPC},
year = {2023},
isbn = {9798400700156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572848.3577436},
doi = {10.1145/3572848.3577436},
abstract = {Containerization approaches based on namespaces offered by the Linux kernel have seen an increasing popularity in the HPC community both as a means to isolate applications and as a format to package and distribute them. However, their adoption and usage in HPC systems faces several challenges. These include difficulties in unprivileged running and building of scientific application container images directly on HPC resources, increasing heterogeneity of HPC architectures, and access to specialized networking libraries available only on HPC systems. These challenges of container-based HPC application development closely align with the several advantages that a new universal intermediate binary format called WebAssembly (Wasm) has to offer. These include a lightweight userspace isolation mechanism and portability across operating systems and processor architectures. In this paper, we explore the usage of Wasm as a distribution format for MPI-based HPC applications. To this end, we present MPIWasm, a novel Wasm embedder for MPI-based HPC applications that enables high-performance execution of Wasm code, has low-overhead for MPI calls, and supports high-performance networking interconnects present on HPC systems. We evaluate the performance and overhead of MPIWasm on a production HPC system and AWS Graviton2 nodes using standardized HPC benchmarks. Results from our experiments demonstrate that MPIWasm delivers competitive native application performance across all scenarios. Moreover, we observe that Wasm binaries are 139.5x smaller on average as compared to the statically-linked binaries for the different standardized benchmarks.},
booktitle = {Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
pages = {92–106},
numpages = {15},
keywords = {HPC, MPI, WebAssembly, wasm, wasmer},
location = {Montreal, QC, Canada},
series = {PPoPP '23}
}


