\documentclass[sigconf]{acmart}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}


%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% These commands are for a PROCEEDINGS abstract or paper.
\settopmatter{printacmref=true} % Removes citation information below abstract
%% Was false before, we need to check why its not working as intended
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in 

\acmConference[AEPRO 2026]{AEPRO 2026: Algorithm Engineering Projects}{March 1}{Jena, Germany}

% convert text to title case
% http://individed.com/code/to-title-case/

% that helps you to formulate your sentences
% https://www.deepl.com/translator

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%% TODO: think about good title
\title[Image Text Enhancer]{Image Text Enhancer}
\subtitle{\large Algorithm Engineering 2026 Project Paper}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.

\author{Daniel Motz}
\affiliation{%
  \institution{Friedrich Schiller University Jena}
  \city{Jena}
  \country{Germany}}
\email{daniel.motz@uni-jena.de}

\author{Leonard Teschner}
\affiliation{%
  \institution{Friedrich Schiller University Jena}
  \city{Jena}
  \country{Germany}}
\email{leonard.teschnner@uni-jena.de}

\author{Mher Mnatsakanyan}
\affiliation{%
  \institution{Friedrich Schiller University Jena}
  \city{Jena}
  \country{Germany}}
\email{mher.mnatsakanyan@uni-jena.de}

%% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}

The five-finger pattern:
\begin{enumerate}
\item \textbf{Topic and background:} What topic does the paper deal with? What is the point of departure for your research? Why are you studying this now?
\item \textbf{Focus:} What is your research question? What are you studying precisely?
\item \textbf{Method:} What did you do?
\item \textbf{Key findings:} What did you discover?
\item \textbf{Conclusions or implications:} What do these findings mean? What broader issues do they speak to?
\end{enumerate}


\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{noise reduction, background removal, image filter, binarization}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:intro}

\subsection{Background}
\label{sec:background}

In Zeiten der Digitalisierung werden viele gedruckte, handschriftliche und historische Dokumente mit einem Scanner oder der Kamera eines Smartphones digitalisiert. Dabei entstehen häufig Bilder in schlechter Qualität, die sich nicht für digitale Bildverarbeitungsverfahren wie die Texterkennung eignen \cite{Investigation_binarization_techniques_unevenly_illuminated_document_images_alqudah_2015,review_document_binarization_yang_2024,review_degradation_image_enhancement_zhou_2023}.
Probleme bei der Digitalisierung können durch die Aufnahmeart sowie durch den Dokumententyp unterschiedliche Herausforderungen entstehen:
Dokumente, die mit einer Smartphone-Kamera gescannt wurden, können nicht alle Details wie mit einem dedizierten Dokumentenscanner erfassen. Dadurch entstehen verschwommene und verzerrte Bilder. Auch der Winkel und der Abstand der Kamera zum Dokument beeinflussen die Qualität. So können verzerrte Perspektiven und Probleme beim Text-Alignment verursacht werden. Die Lichteigenschaften können Schatten werfen sowie Glare und Reflexionen hervorrufen. Zusätzlich können Smartphone-Kameras Text erfassen, der nicht zum gescannten Dokument selbst gehört \cite{comprehensive_review_document_image_binarization_bataineh_2025, Investigation_binarization_techniques_unevenly_illuminated_document_images_alqudah_2015}.
Gerade bei historischen Dokumenten besteht ein großes Interesse daran, sie zu digitalisieren. So bleiben sie erhalten und können mit Texterkennung analysiert werden. Aufgrund ihres hohen Alters, der Lagerung und der Art und Weise ihrer Erstellung leiden sie jedoch häufig unter Problemen. So können sie Fading und Noise aufweisen, wodurch der Text unlesbar wird. Die Digitalisierung wird durch handschriftliche Notizen, überlappende Texte, stilistische Variationen sowie durch beschädigte Seiten, Risse und Schimmelstellen komplexer  \cite{comprehensive_review_document_image_binarization_bataineh_2025,degraded_historical_document_binarization_sulaiman_2019}.

\subsection{Related Work}
\label{sec:related-work}
\begin{comment}
  Tools die binarisierung bereits können:
  - OpenCV
  - Tesseract
  - Leptonica - von copilopt vorgeschlagen
  - ImageMagick
\end{comment}

\begin{comment}
  Diese Paper wurden nicht von mir gelesen, aber in \cite{review_document_binarization_yang_2024} als beispiel für die verschiedenen Kategorien von Binarisierungsmethoden genannt:
  \cite{Yang2008}, \cite{Lu2010}, cite{Tong2009}
\end{comment}

Bei der Bildverbesserung sowie bei der Analyse digitalisierter Dokumente mithilfe von OCR-Texterkennung und Recognition-Systemen ist die Segmentierung von Hintergrund und Vordergrund ein wichtiger Schritt \cite{review_document_binarization_yang_2024}. Die Segmentierung wird mit der Binarisierung umgesetzt. Bei der Binarisierung gibt es traditionelle Methoden, welche einen globalen \cite{Otsu1979}, lokalen \cite{adaptive_thresholding_methods_bataineh_2011, adaptive_thresholding_integral_image_bradley_roth_2007, image_binarization_sauvola_2000} oder auch gemixeten Threshold berechnen \cite{Yang2008, review_document_binarization_yang_2024} und pixelweise mit ihm die Pixel als Vorder- und Hintergrund klassifizieren. Mit Image Feature Methoden wurden Edge Detection \cite{Lu2010, review_document_binarization_yang_2024} und Fuzzy Logics \cite{Tong2009, review_document_binarization_yang_2024} für die Binarisierung eingesetzt. In den letzten Jahren sind zu den traditionellen Methoden zusätzlich Deep Learning Binarisierungsmethoden hinzugekommen. Diese beruhen auf Convolutional Neural Networks, Generative Adversarial Networks oder Attention Mechanisms. \cite{review_document_binarization_yang_2024}. 

Neben den reinen Binarisierungsmethoden gibt es Ansätze, die mehrere Bildverarbeitungsmethoden kombinieren, um die Qualität von Textbildern zu verbessern. So stellt Alqudah et al. \cite{Investigation_binarization_techniques_unevenly_illuminated_document_images_alqudah_2015} eine Pipeline vor, die Entropiefilter und morphologische Operationen mit Binarisierung kombiniert. In \cite{image_binarization_end_to_end_text_understanding_milyaev_2013} wird Niblacks Binarisierung mit einem Laplace Kantenfilter und globaler Optimierung kombiniert. Ein weiterer Ansatz wird von Vlasceanu et al.   \cite{voting_method_image_binarization_vlasceanu_2022} beschrieben. Dabei werden verschiedene Binarisierungsmethoden kombiniert und mittels eines Voting-Mechanismus entschieden, welcher Pixel dem Vorder- bzw. Hintergrund zuzuordnen ist.

\begin{comment}
  Noch etwas für Denoising und andere bildverbesserungsmethoden, wie deskewing, erosion/dilation, despeckle etc.?
\end{comment}

\subsection{Our Contribution}
\label{sec:our-contribution}

In diesem Paper stellen wir eine Pipeline vor, die mehrere Bildverarbeitungsmethoden kombiniert, um die Qualität von gescannten oder fotografierten Textbildern zu verbessern. 
Sie umfasst Schritte wie Deskewing, Kontrastverbesserung, Rauschunterdrückung, Binarisierung, Despeckle, und morphologische Operationen. Das binarisierte Ergebnis der Pipeline kann außerdem zurück in ein Farbbild konvertiert werden.

Die Pipeline ist modular und benutzerfreundlich aufgebaut. Nutzer können auswählen, welche Schritte sie anwenden möchten, um die Pipeline an ihre spezifischen Anwendungsfälle anzupassen. Die Methodenparameter können einzeln angepasst werden, sind aber standardmäßig mit den State-of-the-Art-Werten konfiguriert, um die Benutzerfreundlichkeit zu gewährleisten.

Uns ist die effiziente Verarbeitung in der Pipeline wichtig. Daher vergleichen wir uns mit traditionellen open-source Binarisierungsmethoden und Bildverbesserungsverfahren und lassen bewusst Deep Learning Ansätze, sowie zahlungspflichtige Software außen vor.

Wir stellen die Pipline als C++ Bibliothek, zusätzlich mit den einzelnen Methoden zur Verfügung. Die Implementierung nutzt parallelisierung durch OpenMP, sowie weitere Optimierungsmethoden wie Loop-blocking um eine effiziente Verarbeitung großer Bilddatenmengen zu gewährleisten. 

\subsection{Outline}
\label{sec:outline}

\begin{comment}
  welche experimente werden wir durchführen?
\end{comment}

This paper is structured as follows: Section \ref{sec:pipeline} provides a detailed description the developed pipeline and its methods. Section \ref{sec:experiments} demonstrates the performance of our pipeline using experiments. Finally, Section \ref{sec:conclusions} summarises our results and provides an outlook on possible future work.

\section{The Pipeline}
\label{sec:pipeline}

Many approaches and best practices already exist for improving the quality of scanned images as seen in section \ref{sec:related-work}. We have developed a pipeline that combines several of these methods in order to achieve potentially good results. Users can choose which steps to apply from the pipeline. The individual methods of the pipeline are shown in algorithm \ref{alg:enhance}.
 
\begin{algorithm}
  \caption{Image Text Enhance Pipeline}
  \label{alg:enhance}
  \begin{enumerate}
  \item convert image to grayscale
  \item Deskew (if requested)
  \item Contrast enhancement
  \item Denoising
  \item Binarization
  \item Despeckle (if requested)
  \item Morphological operations (if requested)
  \item Color passthrough
\end{enumerate}
\end{algorithm}

The individual methods of the pipeline are explained in more detail below.

\subsection{Convert image to graysacle}
\label{subsec:grayscale}

All pipeline methods work on grayscale images. Therefore, the first step is to convert the input image into a grayscale image. This is achieved by applying the weighted sum $Y = 0.299R + 0.587G + 0.114B$, as defined by the International Telecommunication Union \cite{ITU-R_BT601}, to each pixel. The result is a grayscale image in which the brightness value of each pixel, represented by $Y$, corresponds to that of the original RGB-pixel.
All steps in the pipeline are performed on the converted grayscale image in-place after the conversion.

\subsection{Deskew}
\label{subsec:deskew}

Damit der Text im Bild horizontal ausgerichtet ist, wird ein Deskew Schritt auf den wunsch des Nutzers hin durchgeführt. Besonders text analysis Methoden wie OCR profitieren von horitonal ausgerichteten Texten \cite{novel_adaptive_deskewing_algorithm_document_images_bao_2022}. Unser Deskew Algorithmus nutzt die Projection Profile Methode, die ähnlich zu \cite{automated_entry_system_printed_documents_akiyama_hagita_1990} ist. Zuerst wird das Bild in Graustufen kovertiert und mittels Sauvols Methode Binarisiert\footnote{siehe Abschnitt \ref{subsec:binarization} oder \cite{image_binarization_sauvola_2000}}. Im zweiten Schritt wird der Winkel gesucht, der die horizontale projections Varianz maximiert. Abschließend wird das Bild rotiert, um den Skew zu korrigieren. 

Die Vorteile dieser Methode sind, dass sie Polarity-safe ist\footnote{erkennt hellen vs. dunklen Hintergrund}, eine coarse-to-fine Winkelsuche für Effizienz nutzt und Neumann Randbedingungen verwendet, um schwarze Ecken zu vermeiden.

\subsection{Contrast enhancement}
\label{subsec:contrast-enhancement}

Um die unterschiedlichen Lichtverhältnisse und daraus entstandenen Kontrastprobleme bei der Digitalisierung von Dokumenten zu adressieren, wird ein Kontrasverbesserungsschritt durchgeführt. Dies hilft der Binarisierung im weiteren Verlauf \cite{Investigation_binarization_techniques_unevenly_illuminated_document_images_alqudah_2015}. Dabei wird ein robuster linearer Kontrast Streching Algorithmus angewendet. Dabei werden die unteren 1\% und oberen 1\% der Intensitäten abgeschnitten, um Ausreißer zu ignorieren. Der verbleibende Bereich wird dann auf den vollen Bereich von 0 bis 255 gestreckt.

\subsection{Denoising}
\label{subsec:denoising}

\begin{comment}
  - adaptive Gaussian blur
      Paper fehlt
  - adaptive median blur
      Paper fehlt
\end{comment}

Digitale Bilder sind durch ihre Aufnahmeart, Kompression oder durch ihren Transmissionchannel mit Rauschen vershen, bei der Bildinformationen verloren gehen. Durch das Vorhandensein von Rauschen sind unteranderem Bild analysis Schritte in ihrer effektivität eingeschränkt \cite{review_image_denoising_techniques_fan_2019}. Daher wird in der Pipeline ein Denoising Schritt durchgeführt, der das Rauschen reduziert. Es stehen verschiedene einfache und adaptive Filtermethoden zur Verfügung, die der Nutzer auswählen kann. 
Es werden zwei einfache Filtermethoden angeboten, die von CImg bereitgestellt werden: Ein Gaussian Filter mit Neumann Randbedingungen und ein nicht-linearer Median Filter \cite{cimg}. Der Gauß Filter ist Teil der Low Pass-Filter und zeichnet damit das bild weich. Der Median Filter ist ein nicht-linearer Filter und nimmt den Pixelwert an, der den Median der Nachbarschaftspixel im Fenster darstellt \cite{Image_engineering_zhang_2017}. Zusätzlich zu den beiden einfachen Filtern werden zwei adaptive Filtermethoden angeboten: Ein adaptiver Gaussian Blur Filter, der eine variable Standardabweichungen nutzt, um an Kanten weniger zu verwischen und in flachen Regionen, mit hoher Varianz, stärker zu verwischen. Zusätzlich zum einfachen Median Filter wird ein adaptiver Median Filter bereitgestellt, der besonders gut für die Entfernung von Impulsrauschen (salt-and-pepper) geeignet ist, während Kanten und feine Details erhalten bleiben. Dabei wird mit einem $3\dot3$ Fenster begonnen, das bei der Erkennung von Impulsrauschen bis zu einer definierten maximalen Fenstergröße erweitert wird, während nicht-Impuls-Pixel unverändert bleiben. Dies macht ihn ideal für die Entfernung von Scan-Speckle in Textbildern.

\subsection{Binarization}
\label{subsec:binarization}

Die Segmentierung von Vorder- und Hintergrund ist ein guter Ansatz um die Bildqualität zu verbessern und stellt zudem den ersten Schritt bei Recognision Systemen wie OCR\cite{review_document_binarization_yang_2024,Investigation_binarization_techniques_unevenly_illuminated_document_images_alqudah_2015}. Da die Binarisierungsmethoden in der Literatur gut untersucht sind, bieten wir mehrere Binarisierungsmethoden an, die der Nutzer auswählen kann. Alle Methoden sind Thresholding Methoden, die für jeden Pixel einen Trehshold $T$ berechnen ($T_g$ für globale und $T_w$ für lokale Thresholds) und den Pixelwert $i(x,y)$ mit dem Threshold vergleichen. Diese Art der Binarisierung ist simpel und effizient \cite{comprehensive_review_document_image_binarization_bataineh_2025}. 

Die einfachste Methode ist die Otsu Methode \cite{Otsu1979}, welche einen globalen Threshold berechnet. Dabei wird die intra-class varianze minimiert. Dies ist in der Gleichung \eqref{eq:1} und \eqref{eq:2} dargestellt, wobei $w1(t)$ und $w2(t)$ die Wahrscheinlichkeiten der zwei Klassen (Vorder- und Hintergrund) sind und $\sigma^2\_1(t)$ und $\sigma^2\_2(t)$ die Varianzen der zwei Klassen sind. Dies macht die Methode effizient, aber anfällig für Überlappungen und schlechte Intensitätsverteilungen \cite{comprehensive_review_document_image_binarization_bataineh_2025}.

\begin{gather}
  \sigma^2\_w(t) = w1(t) * \sigma^21(t) + w2(t) * \sigma^2\_2(t) \label{eq:1} \\
  T_g=\underset{t}{\operatorname{argmin}}\ \sigma^2\_w(t) \label{eq:2}
\end{gather}

\begin{comment}
  R = max stddev value (typically 128 for 8-bit images)
\end{comment}

Ein lokaler Trehshold ansatz, der adaptiv ist, ist die Sauvola Methode \cite{image_binarization_sauvola_2000}. Dabei wird für jedes Pixel ein lokaler Threshold $T_w$ in einem Fenster um das Pixel berechnet. Diese Methode ist resistent gegen ungleichmäßige Beleuchtung. Es wird der lokale Mittelwert $m_w$ und die lokale Standardabweichung $\sigma_w$ des Fensters, sowie ein Parameter $R$, welcher die dynamische Range der Standardabweichung darstellt, genutzt. Mit dem Parameter $k$ kann die Sensitivität des Thresholds korrigiert werden. Der Threshold wird in Gleichung \eqref{math:sauvola} dargestellt \cite{review_document_binarization_yang_2024,image_binarization_sauvola_2000}. Wir haben noch den optionalen Paramter $delta$ hinzugefügt, um den Threshold weiter feinjustieren zu können.
\begin{gather}
  T_w = m_w * \left(1 + k * \left(\frac{\sigma_w}{R} - 1\right)\right) - delta \label{math:sauvola}
\end{gather}

\begin{comment}
  ToDo: evtl. noch k in die T Formel einbauen, um die Sensitivität anzupassen
\end{comment}

Eine weitere adaptive Methode, die adaptive die benötigte Fenstergröße anpasst, ist die Methode von Bataineh et al. \cite{adaptive_thresholding_methods_bataineh_2011}. Diese Methode berechnet zuerst einen globalen Threshold $T_{con}$ \eqref{math:adapt_win:T_con}, der den Pixelwert in Vordergrund (black), Hintergrund (weiß) und Confusion-Values (red) klassifiziert \eqref{math:adapt_win:I}. Basierend auf dem Verhältnis von Vordergrund zu Confusion-Values und der globalen Standardabweichung wird eine primäre Fenstergröße $PW_{size}$ ausgewählt \eqref{math:adapt_win:PW}. Sollte die Anzahl der Confusion-Values die der Vordergrundpixel in dem Fenster übersteigen, wird die halbe Fenstergröße $SW_{size}$ verwendet. 

\begin{gather}
  T_{con} = m_g - \frac{m_g^2 * \sigma_g}{(m_g + \sigma_g) * (0.5 max_{level} + \sigma_g)}\label{math:adapt_win:T_con}\\
  I = \begin{cases}
    \text{black}, &  i(x,y) \leq T_{con} - \left(\frac{\sigma_g}{2}\right),\\
    \text{red}, &  T_{con} - \left(\frac{\sigma_g}{2}\right) < i(x,y) < T_{con} + \left(\frac{\sigma_g}{2}\right),\\
    \text{white}, &  i(x, y) \geq T_{con} + \left(\frac{\sigma_g}{2}\right),\\
  \end{cases} \label{math:adapt_win:I}\\
  PW_{size} = \begin{cases}
    \left(\frac{I_h}{4}, \frac{I_w}{6}\right), &  \geq 2.5 \ \text{ or } \ (\sigma_g < 0.1 * max_{level}),\\
    \left(\frac{I_h}{30}, \frac{I_w}{20}\right), &  1 < p < 2-5 \ \text{or} \ (I_h + I_w < 400),\\
    \left(\frac{I_h}{40}, \frac{I_w}{30}\right), & p \leq 1,
  \end{cases}\label{math:adapt_win:PW}
\end{gather}

Anschließend wird für jedes Fenster der lokale Threshold $T_w$ berechnet \eqref{math:bataineh}. Der Threshold nutzt einen adaptiven Standardabweichungswert $\sigma_{adaptive}$, der auf den maximalen und minimalen Wert der Standardabweichung aller Fenster basiert \eqref{math:bataineh:adapt} \cite{adaptive_thresholding_methods_bataineh_2011}. Durch die adaptive Fenstergröße und den adaptiven Thresholdwert basierend auf den Bildmerkmalen, ist diese Methode robust gegenüber unterschiedlichen Herausforderungen wie dünnen Stiftstrichen und kontrastarmen Bildern, es bleibt unvermeidlich ein übermäßiger Hintergrund erhalten \cite{review_document_binarization_yang_2024}.

\begin{gather} 
  T_w = m_w - \frac{m_W^2-\sigma_W}{(m_g+\sigma_W) \times (\sigma_{adaptive}+\sigma_W)}\label{math:bataineh} \\
  \sigma_{adaptive} = \frac{\sigma_W-\sigma_{min}}{\sigma_{max}-\sigma_{min}} \label{math:bataineh:adapt}
\end{gather}

\subsection{Despeckle}
\label{subsec:despeckle}

Um kleine Flecken, die bei der Binarisierung entstehen bzw. übrigbleiben zu entfernen, wird ein Despeckle Schritt angeboten. Dabei werden kleine zusammenhängende Komponenten (speckles) aus dem binarisierten Bild in-place entfernt. Komponenten, die kleiner als ein definierter Schwellenwert sind, werden entfernt.
Für die detektion der zusammenhängenden Komponenten wird die $get\_label\text{()}$ Methode aus der CImg Bibliothek verwendet\cite{cimg}. Diese berechnet die zusammenhängenden Komponenten mittels Hesselinks et al. Algorithmus \cite{connected_components_hesselink_meijster_bron_2001}.

\subsection{Morphological operations}
\label{subsec:morphological-operations}

Nach der Segmentierung von Vorder- und Hintergrund können kleine Löcher oder Inseln entstehen, welche durch Opening und Closing Operationen entfernt werden können \cite{image_engineering_vl2_zhang_2017}. Wir bieten in unserer Pipeline die Möglichkeit, die morphologischen Operationen Dilation und Erosion anzuwenden. Dilation erweitert helle (weiße) Bereiche. Bei binären Bildern kann dies gebrochene Zeichen verbinden oder Striche verdicken. Erosion verkleinert helle Bereiche (erweitert dunkle Bereiche). Bei binären Bildern kann dies kleine Rauschpunkte entfernen oder Striche dünner machen \cite{Digitale_Bildverarbeitung_werner_2020,image_engineering_vl2_zhang_2017}.

\subsection{Color passthrough}
\label{subsec:color-passthrough}

Im letzten Schritt der Pipeline kann das binarisierte Bild genutzt werden, um die Farbwerte des Originalbildes zu erhalten. Dazu wird das binarisierte und verbesserte Bild als Maske verwendet. Alle Pixel, die im binarisierten Bild als Vordergrund (schwarz) klassifiziert wurden, werden durch die Farbe des darunterliegenden Pixels im Originalbild ersetzt. In Gleichung \eqref{math:color} beschreibt $I'(x, y, z)$ das Ergebnis, $I_{original}(x, y, z)$ das farbige Originalbild und $i(x, y)$ das binarisierte Bild.

\begin{gather}
  I'(x, y, z) =
  \begin{cases}
    I_{original}(x, y, z), & \text{if }i(x, y) == \text{schwarz},\\
    \text{white}, & \text{else} 
  \end{cases} \label{math:color}
\end{gather}

\section{Experiments}
\label{sec:experiments}

\begin{comment}
  In der Literatur wird die Binarisierung durch Benchmark Datasets evaluiert, die ground truth Bilder enthalten. Es ergibt aber keinen Sinn für unserer Pipeline diese zu nutzen, da wir nicht nur binarisieren, sondern auch andere Schritte durchführen.
\end{comment}

\begin{comment}
  Wir werden die performance (Laufzeit) der Pipeline anhand verschiedener Einstellungen testen. Bspw. unterschiediche Anzahl an Threads, verschieden große Bilder, verschiedene Kombinationen von Methoden.
\end{comment}


\section{Conclusions}
\label{sec:conclusions}


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{literature}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
